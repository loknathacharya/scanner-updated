"""
Backtesting API Module for integrating BackTestEngine with FastAPI backend

This module provides REST API endpoints for running backtests on signals
generated by the scanner system, eliminating the need for manual file uploads.
"""

import pandas as pd
import numpy as np
from typing import List, Dict, Optional, Any, Union
from typing_extensions import Literal
from fastapi import APIRouter, HTTPException, Depends
from pydantic import BaseModel, validator
import logging
from datetime import datetime
import traceback

# Helper function to convert numpy types to Python native types for JSON serialization
def convert_numpy_types(obj):
    """Convert numpy types to Python native types for JSON serialization"""
    if isinstance(obj, dict):
        return {k: convert_numpy_types(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    elif isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.bool_):
        return bool(obj)
    elif pd.isna(obj):
        return None
    else:
        return obj

# Import required modules
# Import BackTestEngine functions with proper error handling
def _import_backtest_functions():
    """Import BackTestEngine functions with fallback"""
    import logging as _logging
    _local_logger = _logging.getLogger(__name__)
    try:
        from BackTestEngine import (
            run_backtest,
            run_vectorized_single_backtest,
            run_vectorized_parameter_optimization,
            calculate_performance_metrics,
            calculate_leverage_metrics,
            calculate_invested_value_over_time,
            run_single_parameter_combo
        )
        return {
            'run_backtest': run_backtest,
            'run_vectorized_single_backtest': run_vectorized_single_backtest,
            'run_vectorized_parameter_optimization': run_vectorized_parameter_optimization,
            'calculate_performance_metrics': calculate_performance_metrics,
            'calculate_leverage_metrics': calculate_leverage_metrics,
            'calculate_invested_value_over_time': calculate_invested_value_over_time,
            'run_single_parameter_combo': run_single_parameter_combo
        }
    except ImportError as e:
        _local_logger.warning(f"Failed to import BackTestEngine functions: {e}")
        
        # Fallback mock functions
        def mock_run_backtest(ohlcv_df, signals_df, holding_period, stop_loss_pct, take_profit_pct=None,
                            one_trade_per_instrument=False, initial_capital=100000, sizing_method='equal_weight',
                            sizing_params=None, signal_type='long', allow_leverage=False):
            return pd.DataFrame(), []
        
        def mock_run_vectorized_single_backtest(ohlcv_df, signals_df, holding_period, stop_loss_pct,
                                             take_profit_pct=None, one_trade_per_instrument=False,
                                             initial_capital=100000, sizing_method='equal_weight',
                                             sizing_params=None, signal_type='long', allow_leverage=False):
            return pd.DataFrame(), []
        
        def mock_run_vectorized_parameter_optimization(ohlcv_df, signals_df, holding_periods, stop_losses,
                                                     take_profits=None, one_trade_per_instrument=False,
                                                     initial_capital=100000, sizing_method='equal_weight',
                                                     sizing_params=None, signal_type='long',
                                                     use_multiprocessing=True, max_workers=None, allow_leverage=False):
            return pd.DataFrame()
        
        def mock_calculate_performance_metrics(trade_log_df, initial_capital=100000, risk_free_rate=0.06):
            return {}
        
        def mock_calculate_leverage_metrics(trade_log_df, initial_capital=100000):
            return {}
        
        def mock_calculate_invested_value_over_time(trade_log_df):
            return pd.DataFrame()
        
        def mock_run_single_parameter_combo(args):
            return {'trades': [], 'performance_metrics': {}, 'summary': {}}
        
        return {
            'run_backtest': mock_run_backtest,
            'run_vectorized_single_backtest': mock_run_vectorized_single_backtest,
            'run_vectorized_parameter_optimization': mock_run_vectorized_parameter_optimization,
            'calculate_performance_metrics': mock_calculate_performance_metrics,
            'calculate_leverage_metrics': mock_calculate_leverage_metrics,
            'calculate_invested_value_over_time': mock_calculate_invested_value_over_time,
            'run_single_parameter_combo': mock_run_single_parameter_combo
        }

# Import the functions
backtest_functions = _import_backtest_functions()

# Import other modules
try:
    from utils_module import DataProcessor
    from performance_optimizer import PerformanceOptimizer
    from signal_transformer import SignalTransformer
    from backtest_optimizer import BacktestOptimizer
    from monte_carlo import MonteCarloSimulator
    from risk_management import RiskManager
    from backtest_cache import get_backtest_cache
    from backtest_monitoring import get_backtest_monitor
except ImportError:
    from utils_module import DataProcessor
    from performance_optimizer import PerformanceOptimizer
    from signal_transformer import SignalTransformer
    from backtest_optimizer import BacktestOptimizer
    from monte_carlo import MonteCarloSimulator
    from risk_management import RiskManager
    from backtest_cache import get_backtest_cache
    from backtest_monitoring import get_backtest_monitor

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

router = APIRouter()

# Pydantic models for API requests and responses
class BacktestRequest(BaseModel):
    """Request model for backtest execution"""
    signals_data: List[Dict[str, Any]]
    ohlcv_data: List[Dict[str, Any]]
    initial_capital: float = 100000
    stop_loss: float = 5.0
    take_profit: Optional[float] = None
    holding_period: int = 20
    signal_type: str = "long"
    position_sizing: str = "equal_weight"
    allow_leverage: bool = False
    risk_management: Optional[Dict[str, Any]] = None

class BacktestResponse(BaseModel):
    """Response model for backtest results"""
    trades: List[Dict[str, Any]]
    performance_metrics: Dict[str, Any]
    equity_curve: List[Dict[str, Any]]
    summary: Dict[str, Any]
    execution_time: float
    signals_processed: int

class Signal(BaseModel):
    symbol: str
    date: str

class BacktestOptimizationRequest(BaseModel):
    """Enhanced request model for parameter optimization with step functions"""
    signals_data: List[Signal]
    ohlcv_data: List[Dict[str, Any]]
    param_ranges: Dict[str, List[Optional[float]]]
    initial_capital: float = 100000
    stop_loss: float = 5.0
    take_profit: Optional[float] = None
    holding_period: int = 20
    signal_type: str = "long"
    position_sizing: str = "equal_weight"
    allow_leverage: bool = False
    risk_management: Optional[Dict[str, Any]] = None
    # New optimization options
    use_multiprocessing: bool = True
    use_vectorized: bool = True
    max_workers: Optional[int] = None

    @validator('param_ranges')
    def validate_param_ranges(cls, v):
        for key, value in v.items():
            if not isinstance(value, list):
                raise ValueError(f'Parameter range for {key} must be a list')
            for item in value:
                if item is not None and not isinstance(item, (int, float)):
                    raise ValueError(f'Invalid value in {key}: {item} is not a number or None')
        return v

    class Config:
        """Pydantic configuration to allow extra fields and flexible validation"""
        extra = "allow"  # Allow extra fields
        validate_assignment = True

class BacktestOptimizationResponse(BaseModel):
    """Enhanced response model for parameter optimization results"""
    best_params: Dict[str, Any]
    best_performance: Dict[str, Any]
    all_results: List[Dict[str, Any]]
    execution_time: float
    signals_processed: int
    optimization_stats: Dict[str, Any]
    risk_warnings: Optional[List[str]] = None

# Using shared SignalTransformer from signal_transformer module

# Simple adapter class - use imported functions directly
class BacktestEngineAdapter:
    """Adapter class to integrate BackTestEngine with existing performance optimizer"""
    
    def __init__(self):
        self.performance_optimizer = PerformanceOptimizer()
        self.functions = backtest_functions
    
    def optimize_backtest_operations(self, operations: List[Dict[str, Any]], data: pd.DataFrame) -> pd.DataFrame:
        """Apply vectorized operations for better performance"""
        try:
            return self.performance_optimizer.vectorize_operations(operations, data)
        except Exception as e:
            logger.error(f"Error optimizing backtest operations: {str(e)}")
            # Fallback to original data
            return data
    
    def optimize_memory_usage(self, data: pd.DataFrame) -> pd.DataFrame:
        """Optimize memory usage for large datasets"""
        try:
            return self.performance_optimizer.optimize_memory_usage(data)
        except Exception as e:
            logger.error(f"Error optimizing memory usage: {str(e)}")
            # Fallback to original data
            return data
    
    async def run_backtest(self, request: BacktestRequest, use_cache: bool = True,
                    user_id: Optional[str] = None, correlation_id: Optional[str] = None) -> Dict[str, Any]:
        """Run backtest with optimized performance and proper trade list + metrics"""
        start_time = datetime.now()
        cache_key = None  # Initialize cache_key
        
        # Initialize monitoring
        monitor = get_backtest_monitor()
        
        try:
            # Start execution tracking
            with monitor.track_execution(user_id=user_id, correlation_id=correlation_id) as execution_id:
                # Log backtest start
                monitor.log_backtest_start(
                    execution_id=execution_id,
                    params={
                        'initial_capital': request.initial_capital,
                        'stop_loss': request.stop_loss,
                        'take_profit': request.take_profit,
                        'holding_period': request.holding_period,
                        'signal_type': request.signal_type,
                        'position_sizing': request.position_sizing,
                        'allow_leverage': request.allow_leverage
                    },
                    signals_count=len(request.signals_data)
                )
                
                # Transform input data
                transformer = SignalTransformer()
                signals_df = transformer.transform_scanner_signals(request.signals_data)
                ohlcv_df = transformer.transform_ohlcv_data(request.ohlcv_data)
                
                # Check cache if enabled
                cached_result = None
                if use_cache:
                    try:
                        cache = get_backtest_cache()
                        if cache.is_available():
                            # Generate cache key
                            cache_key = cache.generate_cache_key(signals_df, {
                                'initial_capital': request.initial_capital,
                                'stop_loss': request.stop_loss,
                                'take_profit': request.take_profit,
                                'holding_period': request.holding_period,
                                'signal_type': request.signal_type,
                                'position_sizing': request.position_sizing,
                                'allow_leverage': request.allow_leverage,
                                'risk_management': request.risk_management
                            })
                            
                            # Try to get cached result
                            cached_result = await cache.get_backtest_result(cache_key)
                            if cached_result:
                                logger.info(f"Using cached backtest result for {len(signals_df)} signals")
                                # Add cache metadata to result
                                cached_result['from_cache'] = True
                                cached_result['cache_key'] = cache_key
                                return cached_result
                    except Exception as e:
                        logger.warning(f"Cache check failed: {e}. Proceeding with fresh calculation.")

            # Normalize tickers to a common case for joining
            if 'Ticker' in signals_df.columns:
                signals_df['Ticker'] = signals_df['Ticker'].astype(str).str.upper()
            if 'Ticker' in ohlcv_df.columns:
                ohlcv_df['Ticker'] = ohlcv_df['Ticker'].astype(str).str.upper()

            # Optimize memory usage
            signals_df = self.optimize_memory_usage(signals_df)
            ohlcv_df = self.optimize_memory_usage(ohlcv_df)

            # Diagnostics: intersection of tickers and signal count after filtering
            tickers_prices = set(ohlcv_df['Ticker'].unique().tolist())
            before = len(signals_df)
            signals_df = signals_df[signals_df['Ticker'].isin(tickers_prices)].copy()
            after = len(signals_df)
            logger.info(f"[Backtest] Signals before filter: {before}, after filter with OHLCV tickers: {after}, price tickers: {len(tickers_prices)}")

            # Early exit if no signals remain
            if len(signals_df) == 0:
                execution_time = (datetime.now() - start_time).total_seconds()
                logger.warning("[Backtest] No signals matched available OHLCV tickers; returning empty result.")
                return {
                    'trades': [],
                    'performance_metrics': {},
                    'equity_curve': [],
                    'summary': {
                        'holding_period': request.holding_period,
                        'stop_loss': request.stop_loss,
                        'take_profit': request.take_profit,
                        'signal_type': request.signal_type,
                        'sizing_method': request.position_sizing,
                        'diagnostics': {
                            'signals_before_filter': before,
                            'signals_after_filter': after,
                            'reason_counts': {
                                'no_matching_ticker': before
                            }
                        }
                    },
                    'execution_time': execution_time,
                    'signals_processed': 0
                }

            # One-trade-per-instrument toggle (optional)
            one_trade_per_instrument = False
            if request.risk_management and isinstance(request.risk_management, dict):
                one_trade_per_instrument = bool(request.risk_management.get("one_trade_per_instrument", False))

            # Compute pre-run diagnostics to explain potential zero-trade outcomes
            diagnostics = {
                'signals_before_filter': before,
                'signals_after_filter': after,
                'reason_counts': {
                    'no_matching_ticker': 0,
                    'after_last_price_date': 0,
                    'insufficient_future_bars': 0
                }
            }
            try:
                # Build quick vectorized view for boundary checks
                vectorized_data = SignalTransformer().prepare_vectorized_data(ohlcv_df)

                # Iterate signals cheaply
                for _, sig in signals_df.iterrows():
                    tkr = sig['Ticker']
                    if tkr not in vectorized_data:
                        diagnostics['reason_counts']['no_matching_ticker'] += 1
                        continue

                    prices = vectorized_data[tkr]
                    entry_ord = pd.Timestamp(sig['Date']).toordinal()
                    idxs = np.where(prices[:, 0] >= entry_ord)[0]
                    if len(idxs) == 0:
                        diagnostics['reason_counts']['after_last_price_date'] += 1
                        continue

                    entry_idx = idxs[0]
                    if entry_idx + int(request.holding_period) + 1 > len(prices):
                        diagnostics['reason_counts']['insufficient_future_bars'] += 1
                        continue
            except Exception as dxe:
                logger.warning(f"[Backtest] Diagnostics computation skipped: {dxe}")

            # Map UI sizing synonyms to engine naming
            sizing_map = {
                'kelly': 'kelly_criterion',
                'fixed_dollar': 'fixed_amount',
                'percentage': 'percent_risk',
                'equalweight': 'equal_weight'
            }
            sizing_method = sizing_map.get(str(request.position_sizing).strip(), request.position_sizing)

            # Use vectorized single backtest to obtain actual trades
            run_vec_single = self.functions.get('run_vectorized_single_backtest')
            if run_vec_single is None:
                raise RuntimeError("Vectorized backtest function not available")

            trades_df, _warnings = run_vec_single(
                ohlcv_df=ohlcv_df,
                signals_df=signals_df,
                holding_period=request.holding_period,
                stop_loss_pct=request.stop_loss,
                take_profit_pct=request.take_profit,
                one_trade_per_instrument=one_trade_per_instrument,
                initial_capital=request.initial_capital,
                sizing_method=sizing_method,
                sizing_params=(request.risk_management.get("sizing_params") if request.risk_management else None),
                signal_type=str(request.signal_type or 'long').lower(),
                allow_leverage=request.allow_leverage
            )
            
            # DEBUG: Log the trades_df result
            logger.info(f"DEBUG run_vec_single returned trades_df shape: {trades_df.shape if trades_df is not None else 'None'}")
            if trades_df is not None and not trades_df.empty:
                logger.info(f"DEBUG run_vec_single returned {len(trades_df)} trades")
                logger.info(f"DEBUG run_vec_single trades columns: {trades_df.columns.tolist()}")
                logger.info(f"DEBUG run_vec_single first few trades: {trades_df.head(3).to_dict('records')}")
            else:
                logger.warning("DEBUG run_vec_single returned empty or None trades_df")

            # Fallback: if vectorized returns no trades, try the standard backtest path for diagnosis
            if trades_df is None or trades_df.empty:
                run_std = self.functions.get('run_backtest')
                if run_std is not None:
                    try:
                        std_trades_df, _lw = run_std(
                            ohlcv_df,
                            signals_df,
                            request.holding_period,
                            request.stop_loss,
                            request.take_profit,
                            one_trade_per_instrument,
                            request.initial_capital,
                            sizing_method,
                            (request.risk_management.get("sizing_params") if request.risk_management else None),
                            str(request.signal_type or 'long').lower(),
                            request.allow_leverage
                        )
                        if std_trades_df is not None and not std_trades_df.empty:
                            logger.info("[Backtest] Fallback standard engine produced trades after vectorized returned zero.")
                            trades_df = std_trades_df
                    except Exception as fb_e:
                        logger.warning(f"[Backtest] Fallback standard engine failed: {fb_e}")

            # Compute performance metrics
            calc_metrics = self.functions.get('calculate_performance_metrics')
            performance_metrics: Dict[str, Any] = {}
            equity_curve_records: List[Dict[str, Any]] = []
            if trades_df is not None and not trades_df.empty and calc_metrics is not None:
                metrics = calc_metrics(trades_df, initial_capital=request.initial_capital)
                # Convert numpy types to Python native types for JSON serialization
                raw_metrics = {k: v for k, v in metrics.items() if k != "Equity Curve"}
                performance_metrics = convert_numpy_types(raw_metrics)  # type: ignore
                # Add normalized snake_case duplicates for frontend compatibility
                try:
                    if isinstance(performance_metrics, dict):
                        normalized_keys = {
                            'total_return': performance_metrics.get('Total Return (%)'),
                            'win_rate': performance_metrics.get('Win Rate (%)'),
                            'sharpe_ratio': performance_metrics.get('Sharpe Ratio'),
                            'max_drawdown': performance_metrics.get('Max Drawdown (%)'),
                            'profit_factor': performance_metrics.get('Profit Factor'),
                            'total_trades': performance_metrics.get('Total Trades'),
                        }
                        # Keep only non-null values
                        for _k, _v in list(normalized_keys.items()):
                            if _v is None:
                                normalized_keys.pop(_k, None)
                        performance_metrics.update(normalized_keys)
                except Exception as _norm_err:
                    logger.warning(f"Failed to normalize performance metric keys: {_norm_err}")
                eq = metrics.get("Equity Curve")
                if isinstance(eq, pd.DataFrame):
                    # Reset index and coerce keys to str for JSON typing friendliness
                    eq_records = eq.reset_index().to_dict('records')
                    equity_curve_records = convert_numpy_types([{str(k): v for k, v in rec.items()} for rec in eq_records])  # type: ignore

            # Calculate execution time
            execution_time = (datetime.now() - start_time).total_seconds()

            # Serialize trades
            trades_list = []
            if trades_df is not None and not trades_df.empty:
                # Normalize datetimes to string for JSON
                td = trades_df.copy()
                for c in ['Entry Date', 'Exit Date', 'Signal Date']:
                    if c in td.columns:
                        td[c] = pd.to_datetime(td[c], errors='coerce').dt.strftime("%Y-%m-%d")
                trades_list = convert_numpy_types(td.to_dict('records'))  # type: ignore

            response_data = {
                'trades': trades_list,
                'performance_metrics': performance_metrics,
                'equity_curve': equity_curve_records,
                'summary': {
                    'holding_period': request.holding_period,
                    'stop_loss': request.stop_loss,
                    'take_profit': request.take_profit,
                    'signal_type': request.signal_type,
                    'sizing_method': sizing_method,
                    'diagnostics': diagnostics
                },
                'execution_time': execution_time,
                'signals_processed': len(signals_df)
            }

            # Ensure trades_list is a proper list for logging
            trades_count = len(trades_list) if isinstance(trades_list, list) else 0
            logger.info(f"[Backtest] Completed: {execution_time:.2f}s, signals used: {len(signals_df)}, trades: {trades_count}")
            
            # Log backtest completion with monitoring
            monitor.log_backtest_complete(
                execution_id=execution_id,
                trades_count=trades_count,
                performance_metrics=performance_metrics if isinstance(performance_metrics, dict) else {}
            )
            
            # Log performance metrics
            monitor.log_performance_metrics(execution_id, performance_metrics)
            
            # Record cache operation if used
            if cached_result is not None:
                monitor.record_cache_operation('get', 0, hit=True)
            
            # Add monitoring metadata to response
            response_data['monitoring'] = {
                'execution_id': execution_id,
                'cache_hit': cached_result is not None,
                'cache_key': cache_key if cached_result else None,
                'from_cache': cached_result is not None
            }
            
            # Cache the result if caching is enabled and Redis is available
            if use_cache and cached_result is None:
                try:
                    cache = get_backtest_cache()
                    if cache.is_available():
                        # Generate cache key (same as above)
                        cache_key = cache.generate_cache_key(signals_df, {
                            'initial_capital': request.initial_capital,
                            'stop_loss': request.stop_loss,
                            'take_profit': request.take_profit,
                            'holding_period': request.holding_period,
                            'signal_type': request.signal_type,
                            'position_sizing': request.position_sizing,
                            'allow_leverage': request.allow_leverage,
                            'risk_management': request.risk_management
                        })
                        
                        # Cache the result and record operation
                        await cache.set_backtest_result(cache_key, response_data, result_type='standard')
                        monitor.record_cache_operation('set', 0)  # Actual timing would be measured in cache
                        logger.info(f"Cached backtest result for {len(signals_df)} signals")
                except Exception as e:
                    logger.warning(f"Failed to cache result: {e}")
            
            return response_data

        except Exception as e:
            execution_time = (datetime.now() - start_time).total_seconds()
            logger.error(f"Backtest failed after {execution_time:.2f}s: {str(e)}")
            logger.error(traceback.format_exc())
            raise HTTPException(status_code=500, detail=f"Backtest execution failed: {str(e)}")

# Dependency injection
def get_backtest_adapter() -> BacktestEngineAdapter:
    """Get backtest engine adapter instance"""
    return BacktestEngineAdapter()

@router.post("/run", response_model=BacktestResponse)
async def run_backtest(
    request: BacktestRequest,
    adapter: BacktestEngineAdapter = Depends(get_backtest_adapter),
    use_cache: bool = True,
    user_id: Optional[str] = None,
    correlation_id: Optional[str] = None
):
    """
    Run backtest on provided signals using the same uploaded dataset for signals and OHLCV.
    
    Args:
        request: Backtest request with signals and parameters
        adapter: Backtest engine adapter
        use_cache: Whether to use Redis caching (default: True)
        user_id: User identifier for activity tracking (optional)
        correlation_id: Correlation ID for request tracing (optional)
    """
    try:
        # Validate risk-related configuration before execution
        risk_mgr = RiskManager()
        risk_warnings = risk_mgr.validate_config({
            "holding_period": request.holding_period,
            "stop_loss": request.stop_loss,
            "take_profit": request.take_profit,
            "position_sizing": request.position_sizing,
            "allow_leverage": request.allow_leverage,
            "risk_management": request.risk_management or {}
        })

        results = await adapter.run_backtest(
            request,
            use_cache=use_cache,
            user_id=user_id,
            correlation_id=correlation_id
        )

        # Merge risk warnings into summary if present
        if risk_warnings:
            summary = results.get("summary", {}) or {}
            existing = summary.get("risk_warnings", [])
            summary["risk_warnings"] = list({*existing, *risk_warnings})
            results["summary"] = summary

        return BacktestResponse(**results)
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Unexpected error in backtest endpoint: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Unexpected error: {str(e)}")

@router.post("/optimize", response_model=BacktestOptimizationResponse)
async def optimize_backtest_parameters(
    request: BacktestOptimizationRequest,
    adapter: BacktestEngineAdapter = Depends(get_backtest_adapter)
):
    """
    Enhanced parameter optimization with step functions and multiprocessing support.
    
    This endpoint runs multiple backtests with different parameter combinations
    to find the optimal settings for the given signals.
    """
    start_time = datetime.now()
    try:
        # Transform input data
        transformer = SignalTransformer()
        signals_df = transformer.transform_scanner_signals(request.signals_data)
        ohlcv_df = transformer.transform_ohlcv_data(request.ohlcv_data)
        vectorized_data = transformer.prepare_vectorized_data(ohlcv_df)

        # Optimize memory usage
        signals_df = adapter.optimize_memory_usage(signals_df)
        ohlcv_df = adapter.optimize_memory_usage(ohlcv_df)

        # Build base configuration
        base_cfg = {
            "initial_capital": request.initial_capital,
            "signal_type": request.signal_type,
            "position_sizing": request.position_sizing,
            "allow_leverage": request.allow_leverage,
            "one_trade_per_instrument": request.risk_management.get("one_trade_per_instrument", False) if request.risk_management else False,
            "sizing_params": (request.risk_management.get("sizing_params") if request.risk_management else {}) or {},
            "holding_period": request.holding_period,
            "stop_loss": request.stop_loss,
            "take_profit": request.take_profit,
        }

        # Risk config validation (non-blocking warnings)
        risk_mgr = RiskManager()
        risk_warnings = risk_mgr.validate_config({
            "holding_period": request.holding_period,
            "stop_loss": request.stop_loss,
            "take_profit": request.take_profit,
            "position_sizing": request.position_sizing,
            "allow_leverage": request.allow_leverage,
            "risk_management": request.risk_management or {}
        })

        # Use the enhanced BacktestOptimizer
        optimizer = BacktestOptimizer(adapter.functions)
        result_bundle = optimizer.optimize_parameters(
            signals_df=signals_df,
            vectorized_data=vectorized_data,
            base_cfg=base_cfg,
            param_ranges=request.param_ranges or {},
            max_workers=request.max_workers,
            use_multiprocessing=request.use_multiprocessing,
            use_vectorized=request.use_vectorized
        )

        execution_time = (datetime.now() - start_time).total_seconds()

        response_data = {
            'best_params': result_bundle.get('best_params', {}),
            'best_performance': result_bundle.get('best_performance', {}),
            'all_results': result_bundle.get('all_results', []),
            'execution_time': execution_time,
            'signals_processed': len(signals_df),
            'optimization_stats': result_bundle.get('optimization_stats', {}),
            'risk_warnings': risk_warnings or []
        }

        logger.info(f"Enhanced parameter optimization completed: {execution_time:.2f}s, {len(signals_df)} signals processed")
        return BacktestOptimizationResponse(**response_data)
    except Exception as e:
        logger.error(f"Enhanced parameter optimization failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Parameter optimization failed: {str(e)}")

@router.get("/health")
async def health_check():
    """Health check endpoint for backtest API"""
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}


@router.get("/cache/stats")
async def cache_stats():
    """Get Redis cache statistics"""
    try:
        cache = get_backtest_cache()
        stats = await cache.get_cache_stats()
        return stats
    except Exception as e:
        logger.error(f"Failed to get cache stats: {e}")
        return {"status": "error", "error": str(e)}


@router.delete("/cache")
async def clear_cache(pattern: str = "*"):
    """Clear Redis cache entries"""
    try:
        cache = get_backtest_cache()
        deleted_count = await cache.clear_cache(pattern)
        return {"message": f"Cleared {deleted_count} cache entries", "pattern": pattern}
    except Exception as e:
        logger.error(f"Failed to clear cache: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to clear cache: {str(e)}")

class MonteCarloRequest(BaseModel):
    """Request for Monte Carlo simulation"""
    trade_log: List[Dict[str, Any]]
    n_simulations: int = 1000
    n_trades: int = 50
    rng_seed: Optional[int] = None

@router.post("/montecarlo")
async def run_monte_carlo(request: MonteCarloRequest):
    """
    Run Monte Carlo simulation on trade results (percent returns or portfolio returns).
    Accepts trade_log with either 'Profit/Loss (%)' or 'Portfolio Return'.
    """
    try:
        df = pd.DataFrame(request.trade_log or [])
        sim = MonteCarloSimulator(rng_seed=request.rng_seed)
        results = sim.run_simulation(
            trade_log_df=df,
            n_simulations=request.n_simulations,
            n_trades=request.n_trades,
        )
        return results
    except Exception as e:
        logger.error(f"Monte Carlo simulation failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Monte Carlo simulation failed: {str(e)}")

def _generate_param_combinations(param_ranges: Dict[str, List[Union[float, None]]]) -> List[Dict[str, Union[float, None]]]:
    """Generate all parameter combinations for optimization"""
    try:
        from itertools import product
        
        param_names = list(param_ranges.keys())
        param_values = list(param_ranges.values())
        
        combinations = []
        for values in product(*param_values):
            combination = dict(zip(param_names, values))
            combinations.append(combination)
        
        logger.info(f"Generated {len(combinations)} parameter combinations")
        return combinations
    
    except Exception as e:
        logger.error(f"Error generating parameter combinations: {str(e)}")
        raise ValueError(f"Parameter combination generation failed: {str(e)}")

@router.get("/schema")
async def schema():
    """
    Returns example request bodies for Phase 1 integration
    """
    return {
        "run.request": {
            "signals_data": [{"symbol": "RELIANCE", "date": "2023-01-02"}],
            "ohlcv_data": [{
                "symbol": "RELIANCE", "date": "2023-01-02",
                "open": 2500.0, "high": 2550.0, "low": 2480.0, "close": 2520.0, "volume": 1000000
            }],
            "initial_capital": 100000,
            "stop_loss": 5.0,
            "take_profit": 10.0,
            "holding_period": 10,
            "signal_type": "long",
            "position_sizing": "equal_weight",
            "allow_leverage": False,
            "risk_management": {}
        },
        "optimize.request": {
            "signals_data": [{"symbol": "RELIANCE", "date": "2023-01-02"}],
            "ohlcv_data": [{
                "symbol": "RELIANCE", "date": "2023-01-02",
                "open": 2500.0, "high": 2550.0, "low": 2480.0, "close": 2520.0, "volume": 1000000
            }],
            "param_ranges": {
                "stop_loss": [2.0, 5.0, 8.0],
                "take_profit": [0, 10.0, 15.0]
            },
            "initial_capital": 100000,
            "holding_period": 10,
            "signal_type": "long",
            "position_sizing": "equal_weight",
            "allow_leverage": False,
            "risk_management": {}
        }
    }


# Monitoring and Analytics Endpoints

@router.get("/monitoring/health")
async def get_system_health():
    """Get current system health metrics"""
    try:
        monitor = get_backtest_monitor()
        health_data = monitor.get_system_health()
        return {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "system_health": health_data
        }
    except Exception as e:
        logger.error(f"Failed to get system health: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get system health: {str(e)}")


@router.get("/monitoring/cache")
async def get_cache_performance():
    """Get cache performance metrics"""
    try:
        monitor = get_backtest_monitor()
        cache_data = monitor.get_cache_performance()
        return {
            "status": "success",
            "timestamp": datetime.now().isoformat(),
            "cache_performance": cache_data
        }
    except Exception as e:
        logger.error(f"Failed to get cache performance: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get cache performance: {str(e)}")


@router.get("/monitoring/execution/{execution_id}")
async def get_execution_summary(execution_id: str):
    """Get summary of a specific backtest execution"""
    try:
        monitor = get_backtest_monitor()
        summary = monitor.get_execution_summary(execution_id)
        if summary is None:
            raise HTTPException(status_code=404, detail=f"Execution {execution_id} not found")
        return {
            "status": "success",
            "execution_summary": summary
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get execution summary: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get execution summary: {str(e)}")


@router.get("/monitoring/active")
async def get_active_executions():
    """Get currently active backtest executions"""
    try:
        monitor = get_backtest_monitor()
        active_executions = monitor.get_active_executions()
        return {
            "status": "success",
            "timestamp": datetime.now().isoformat(),
            "active_executions": active_executions
        }
    except Exception as e:
        logger.error(f"Failed to get active executions: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get active executions: {str(e)}")


@router.get("/monitoring/analytics")
async def get_performance_analytics(
    days: int = 7,
    user_id: Optional[str] = None
):
    """Get performance analytics for backtest operations"""
    try:
        monitor = get_backtest_monitor()
        analytics = monitor.get_performance_analytics(days=days)
        
        # Filter by user if specified
        if user_id:
            user_activity = monitor.get_user_activity(user_id=user_id)
            analytics['user_activity'] = user_activity
        else:
            analytics['user_activity'] = monitor.get_user_activity(limit=100)
        
        return {
            "status": "success",
            "timestamp": datetime.now().isoformat(),
            "analytics": analytics
        }
    except Exception as e:
        logger.error(f"Failed to get performance analytics: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get performance analytics: {str(e)}")


@router.get("/monitoring/user/{user_id}")
async def get_user_activity(
    user_id: str,
    limit: int = 100
):
    """Get activity history for a specific user"""
    try:
        monitor = get_backtest_monitor()
        activity = monitor.get_user_activity(user_id=user_id, limit=limit)
        return {
            "status": "success",
            "timestamp": datetime.now().isoformat(),
            "user_id": user_id,
            "activity": activity
        }
    except Exception as e:
        logger.error(f"Failed to get user activity: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get user activity: {str(e)}")


@router.delete("/monitoring/data")
async def cleanup_monitoring_data(
    days: int = 30,
    confirm: bool = False
):
    """Clean up old monitoring data"""
    if not confirm:
        raise HTTPException(
            status_code=400,
            detail="Set confirm=true to proceed with data cleanup"
        )
    
    try:
        monitor = get_backtest_monitor()
        monitor.cleanup_old_data(days=days)
        return {
            "status": "success",
            "message": f"Cleaned up monitoring data older than {days} days",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Failed to cleanup monitoring data: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to cleanup monitoring data: {str(e)}")


@router.get("/monitoring/export")
async def export_monitoring_data(
    format: str = "json",
    include_sensitive: bool = False
):
    """Export monitoring data in specified format"""
    try:
        if format.lower() not in ["json"]:
            raise HTTPException(
                status_code=400,
                detail="Unsupported format. Only 'json' is supported."
            )
        
        monitor = get_backtest_monitor()
        exported_data = monitor.export_monitoring_data(format=format)
        
        return {
            "status": "success",
            "format": format,
            "data": exported_data,
            "timestamp": datetime.now().isoformat()
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to export monitoring data: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to export monitoring data: {str(e)}")


@router.get("/monitoring/stats")
async def get_monitoring_stats():
    """Get overall monitoring statistics"""
    try:
        monitor = get_backtest_monitor()
        
        # Get various metrics
        system_health = monitor.get_system_health()
        cache_performance = monitor.get_cache_performance()
        active_executions = monitor.get_active_executions()
        analytics = monitor.get_performance_analytics(days=1)  # Last 24 hours
        
        return {
            "status": "success",
            "timestamp": datetime.now().isoformat(),
            "statistics": {
                "system_health": system_health,
                "cache_performance": cache_performance,
                "active_executions": len(active_executions),
                "recent_analytics": analytics,
                "total_executions": len(monitor.executions_history),
                "total_users": len(monitor.user_activity)
            }
        }
    except Exception as e:
        logger.error(f"Failed to get monitoring stats: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get monitoring stats: {str(e)}")